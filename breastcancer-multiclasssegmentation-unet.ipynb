{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7307909,"sourceType":"datasetVersion","datasetId":4222901}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Install packages**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install segmentation-models-pytorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-06T18:53:29.460093Z","iopub.execute_input":"2025-01-06T18:53:29.460770Z","iopub.status.idle":"2025-01-06T18:53:45.511916Z","shell.execute_reply.started":"2025-01-06T18:53:29.460738Z","shell.execute_reply":"2025-01-06T18:53:45.510556Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. Import libraries**","metadata":{}},{"cell_type":"code","source":"# Data handling\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\n\n# Torch\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport segmentation_models_pytorch as smp\nfrom torchinfo import summary\n\n# os\nimport os\n\n# Path\nfrom pathlib import Path\n\n# tqdm\nfrom tqdm.auto import tqdm\n\n# warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:53:45.514054Z","iopub.execute_input":"2025-01-06T18:53:45.514386Z","iopub.status.idle":"2025-01-06T18:53:51.941076Z","shell.execute_reply.started":"2025-01-06T18:53:45.514345Z","shell.execute_reply":"2025-01-06T18:53:51.940195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Load data**","metadata":{}},{"cell_type":"code","source":"# We define a function to create a list of the paths of the images and masks.\ndef image_mask_path(image_path:str, mask_path:str):\n    IMAGE_PATH = Path(image_path)\n    IMAGE_PATH_LIST = sorted(list(IMAGE_PATH.glob(\"*.png\")))\n\n    MASK_PATH = Path(mask_path)\n    MASK_PATH_LIST = sorted(list(MASK_PATH.glob(\"*.png\")))\n    \n    return IMAGE_PATH_LIST, MASK_PATH_LIST","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:53:51.942122Z","iopub.execute_input":"2025-01-06T18:53:51.942363Z","iopub.status.idle":"2025-01-06T18:53:51.947182Z","shell.execute_reply.started":"2025-01-06T18:53:51.942341Z","shell.execute_reply":"2025-01-06T18:53:51.946260Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path_train = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS_512/train_512\"\nmask_path_train = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS_512/train_mask_512\"\n\nIMAGE_PATH_LIST_TRAIN, MASK_PATH_LIST_TRAIN = image_mask_path(image_path_train, \n                                                              mask_path_train)\n\nprint(f'Total Images Train: {len(IMAGE_PATH_LIST_TRAIN)}')\nprint(f'Total Masks Train: {len(MASK_PATH_LIST_TRAIN)}')","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:53:51.948920Z","iopub.execute_input":"2025-01-06T18:53:51.949168Z","iopub.status.idle":"2025-01-06T18:53:52.449324Z","shell.execute_reply.started":"2025-01-06T18:53:51.949147Z","shell.execute_reply":"2025-01-06T18:53:52.448410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path_val = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS_512/val_512\"\nmask_path_val = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS_512/val_mask_512\"\n\nIMAGE_PATH_LIST_VAL, MASK_PATH_LIST_VAL = image_mask_path(image_path_val, \n                                                          mask_path_val)\n\nprint(f'Total Images Val: {len(IMAGE_PATH_LIST_VAL)}')\nprint(f'Total Masks Val: {len(MASK_PATH_LIST_VAL)}')","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:53:52.450400Z","iopub.execute_input":"2025-01-06T18:53:52.450723Z","iopub.status.idle":"2025-01-06T18:53:52.746330Z","shell.execute_reply.started":"2025-01-06T18:53:52.450700Z","shell.execute_reply":"2025-01-06T18:53:52.745476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VALUES_UNIQUE_TRAIN = []\n\nfor i in MASK_PATH_LIST_TRAIN:\n    sample = cv2.imread(str(i), cv2.IMREAD_GRAYSCALE)\n    uniques = np.unique(sample)\n    VALUES_UNIQUE_TRAIN.append(uniques)\n    \nFINAL_VALUES_UNIQUE_TRAIN = np.concatenate(VALUES_UNIQUE_TRAIN)\nprint(\"Unique values Train:\\n\")\nprint(np.unique(FINAL_VALUES_UNIQUE_TRAIN))","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:53:52.747325Z","iopub.execute_input":"2025-01-06T18:53:52.747610Z","iopub.status.idle":"2025-01-06T18:54:34.961768Z","shell.execute_reply.started":"2025-01-06T18:53:52.747589Z","shell.execute_reply":"2025-01-06T18:54:34.960790Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\n# Colors for labels (for visualization, choose colors you like)\ncolors = plt.cm.tab20(range(22))  # Generate a list of 22 colors\n\n# Labels for the legend\nlabels = [\n    \"outside_roi\", \"tumor\", \"stroma\", \"lymphocytic_infiltrate\", \n    \"necrosis_or_debris\", \"glandular_secretions\", \"blood\", \"exclude\", \n    \"metaplasia_NOS\", \"fat\", \"plasma_cells\", \"other_immune_infiltrate\", \n    \"mucoid_material\", \"normal_acinus_or_duct\", \"lymphatics\", \"undetermined\", \n    \"nerve\", \"skin_adnexa\", \"blood_vessel\", \"angioinvasion\", \"dcis\", \"other\"\n]\n\n# Generate label patches for legend\nlegend_patches = [mpatches.Patch(color=colors[i], label=f\"{i}: {labels[i]}\") for i in range(22)]\n\n# Plot legend in horizontal layout\nfig, ax = plt.subplots(figsize=(12, 2))\nax.axis('off')  # Turn off axes\n\n# Add legend horizontally\nplt.legend(\n    handles=legend_patches,\n    loc='center',\n    bbox_to_anchor=(0.5, 0.5),\n    ncol=5,  # Number of columns in the legend\n    title=\"Label Mapping\"\n)\nplt.title(\"Label Color Mapping\", fontsize=12, fontweight=\"bold\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:34.962864Z","iopub.execute_input":"2025-01-06T18:54:34.963168Z","iopub.status.idle":"2025-01-06T18:54:35.299704Z","shell.execute_reply.started":"2025-01-06T18:54:34.963133Z","shell.execute_reply":"2025-01-06T18:54:35.298835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display 5 images and their respective masks horizontally\nfig, ax = plt.subplots(nrows=2, ncols=5, figsize=(25, 10))\n\nfor i, (img_path, mask_path) in enumerate(zip(IMAGE_PATH_LIST_TRAIN, MASK_PATH_LIST_TRAIN)):\n    \n    if i > 4:  # Limit to 5 images\n        break\n    \n    # Load and display the image\n    img_bgr = cv2.imread(str(img_path))\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    ax[0, i].imshow(img_rgb)\n    ax[0, i].axis('off')\n    ax[0, i].set_title(f\"Image\\nShape: {img_rgb.shape}\", fontsize=10, fontweight=\"bold\", color=\"black\")\n\n    # Load and display the mask\n    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n    ax[1, i].imshow(mask)\n    ax[1, i].axis('off')\n    ax[1, i].set_title(f\"Mask\\nShape: {mask.shape}\", fontsize=10, fontweight=\"bold\", color=\"black\")\n\nfig.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:35.300717Z","iopub.execute_input":"2025-01-06T18:54:35.300958Z","iopub.status.idle":"2025-01-06T18:54:37.280155Z","shell.execute_reply.started":"2025-01-06T18:54:35.300937Z","shell.execute_reply":"2025-01-06T18:54:37.279102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We visualize some images but with the mask superimposed.\nfig, ax = plt.subplots(nrows = 10, ncols = 2, figsize = (12,30))\nax = ax.flat\n\nfor i,(img_path, mask_path) in enumerate(zip(IMAGE_PATH_LIST_TRAIN, MASK_PATH_LIST_TRAIN)):\n    \n    if i>19:\n        break\n        \n    img_bgr = cv2.imread(str(img_path))\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    ax[i].imshow(img_rgb)\n    ax[i].axis('off')\n    \n\n    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n    ax[i].imshow(mask, alpha = 0.30)\n    ax[i].axis('off')\n    \n\nfig.tight_layout()\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:37.281414Z","iopub.execute_input":"2025-01-06T18:54:37.281855Z","iopub.status.idle":"2025-01-06T18:54:41.163190Z","shell.execute_reply.started":"2025-01-06T18:54:37.281817Z","shell.execute_reply":"2025-01-06T18:54:41.161734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"**We will create dataframes for both data sets.**","metadata":{}},{"cell_type":"code","source":"data_train = pd.DataFrame({'Image':IMAGE_PATH_LIST_TRAIN, \n                           'Mask':MASK_PATH_LIST_TRAIN})\n\ndata_val = pd.DataFrame({'Image':IMAGE_PATH_LIST_VAL, \n                         'Mask':MASK_PATH_LIST_VAL})","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.168294Z","iopub.execute_input":"2025-01-06T18:54:41.169058Z","iopub.status.idle":"2025-01-06T18:54:41.217360Z","shell.execute_reply.started":"2025-01-06T18:54:41.169011Z","shell.execute_reply":"2025-01-06T18:54:41.216417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now we are going to find out what transformations were applied to the images when the model was pre-trained in order to replicate it in our images.**","metadata":{}},{"cell_type":"code","source":"preprocess_input = smp.encoders.get_preprocessing_fn(encoder_name = \"resnet34\", \n                                        pretrained = \"imagenet\")\npreprocess_input","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.218243Z","iopub.execute_input":"2025-01-06T18:54:41.218573Z","iopub.status.idle":"2025-01-06T18:54:41.229770Z","shell.execute_reply.started":"2025-01-06T18:54:41.218550Z","shell.execute_reply":"2025-01-06T18:54:41.228932Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We are going to replicate this same thing.**","metadata":{}},{"cell_type":"code","source":"RESIZE = (224, 224)\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\nimage_transforms = transforms.Compose([transforms.Resize(RESIZE),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize(mean = MEAN, std = STD)])\n\nmask_transforms = transforms.Compose([transforms.Resize(RESIZE), \n                                      transforms.PILToTensor()])","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.230925Z","iopub.execute_input":"2025-01-06T18:54:41.231231Z","iopub.status.idle":"2025-01-06T18:54:41.238291Z","shell.execute_reply.started":"2025-01-06T18:54:41.231208Z","shell.execute_reply":"2025-01-06T18:54:41.237597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We define our Dataset with all the transformations to perform.**","metadata":{}},{"cell_type":"markdown","source":"- **Dataset**","metadata":{}},{"cell_type":"code","source":"class CustomImageMaskDataset(Dataset):\n    def __init__(self, data:pd.DataFrame, image_transforms, mask_transforms):\n        self.data = data\n        self.image_transforms = image_transforms\n        self.mask_transforms = mask_transforms\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        image_path = self.data.iloc[idx, 0]\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.image_transforms(image)\n        \n        mask_path = self.data.iloc[idx, 1]\n        mask = Image.open(mask_path)\n        mask = self.mask_transforms(mask)\n        \n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.239381Z","iopub.execute_input":"2025-01-06T18:54:41.239720Z","iopub.status.idle":"2025-01-06T18:54:41.251063Z","shell.execute_reply.started":"2025-01-06T18:54:41.239691Z","shell.execute_reply":"2025-01-06T18:54:41.250294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomImageMaskDataset(data_train, image_transforms, \n                                       mask_transforms)\n    \nval_dataset = CustomImageMaskDataset(data_val, image_transforms, \n                                     mask_transforms)","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.252041Z","iopub.execute_input":"2025-01-06T18:54:41.252346Z","iopub.status.idle":"2025-01-06T18:54:41.261649Z","shell.execute_reply.started":"2025-01-06T18:54:41.252309Z","shell.execute_reply":"2025-01-06T18:54:41.260917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **DataLoader**","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_WORKERS = os.cpu_count()\n\ntrain_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n                              shuffle = True, num_workers = NUM_WORKERS)\n\nval_dataloader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, \n                            shuffle = True, num_workers = NUM_WORKERS)","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.262711Z","iopub.execute_input":"2025-01-06T18:54:41.263013Z","iopub.status.idle":"2025-01-06T18:54:41.269623Z","shell.execute_reply.started":"2025-01-06T18:54:41.262991Z","shell.execute_reply":"2025-01-06T18:54:41.268898Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We visualize the dimensions of a batch.\nbatch_images, batch_masks = next(iter(train_dataloader))\n\nbatch_images.shape, batch_masks.shape","metadata":{"execution":{"iopub.status.busy":"2025-01-06T18:54:41.270394Z","iopub.execute_input":"2025-01-06T18:54:41.270673Z","iopub.status.idle":"2025-01-06T18:54:45.445221Z","shell.execute_reply.started":"2025-01-06T18:54:41.270652Z","shell.execute_reply":"2025-01-06T18:54:45.444218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.UNet++ model","metadata":{}},{"cell_type":"code","source":"# Define UNet++ Model\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=22,\n    activation=None\n)\n\n# Visualize Model Summary\nsummary(\n    model=model,\n    input_size=[BATCH_SIZE, 3, 224, 224],\n    col_width=15,\n    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n    row_settings=['var_names']\n)\n\n# Freeze Encoder Parameters\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n\n# Verify Freezing\nsummary(\n    model=model,\n    input_size=[BATCH_SIZE, 3, 224, 224],\n    col_width=15,\n    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n    row_settings=['var_names']\n)\n\n# Early Stopping Class\nclass EarlyStopping:\n    def __init__(self, patience: int = 5, delta: float = 0.0001, path: str = \"best_model.pth\"):\n        self.patience = patience\n        self.delta = delta\n        self.path = path\n        self.best_score = None\n        self.counter = 0\n        self.early_stop = False\n        \n    def __call__(self, val_loss, model):\n        if self.best_score is None:\n            self.best_score = val_loss\n            self.save_checkpoint(model)\n            \n        elif val_loss >= self.best_score + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                \n        else:\n            self.best_score = val_loss\n            self.save_checkpoint(model)\n            self.counter = 0\n            \n    def save_checkpoint(self, model):\n        torch.save(model.state_dict(), self.path)\n\n# Initialize Early Stopping\nearly_stopping = EarlyStopping(patience=20, delta=0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:45.446629Z","iopub.execute_input":"2025-01-06T18:54:45.446904Z","iopub.status.idle":"2025-01-06T18:54:48.007616Z","shell.execute_reply.started":"2025-01-06T18:54:45.446877Z","shell.execute_reply":"2025-01-06T18:54:48.006921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Training and Validation Steps\ndef train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n    model.train()\n    train_loss = 0.0\n    train_accuracy = 0.0\n    \n    for batch, (X, y) in enumerate(dataloader):\n        X = X.to(device=DEVICE, dtype=torch.float32)\n        y = y.to(device=DEVICE, dtype=torch.long)\n        optimizer.zero_grad()\n        logit_mask = model(X)\n        loss = loss_fn(logit_mask, y.squeeze())\n        train_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n        prob_mask = logit_mask.softmax(dim=1)\n        pred_mask = prob_mask.argmax(dim=1)\n        \n        tp, fp, fn, tn = smp.metrics.get_stats(\n            output=pred_mask.detach().cpu().long(),\n            target=y.squeeze().cpu().long(),\n            mode=\"multiclass\",\n            num_classes=22\n        )\n        \n        train_accuracy += smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").numpy()\n    \n    train_loss /= len(dataloader)\n    train_accuracy /= len(dataloader)\n    \n    return train_loss, train_accuracy\n\ndef val_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, \n             loss_fn: torch.nn.Module):\n    model.eval()\n    val_loss = 0.0\n    val_accuracy = 0.0\n    \n    with torch.inference_mode():\n        for batch, (X, y) in enumerate(dataloader):\n            X = X.to(device=DEVICE, dtype=torch.float32)\n            y = y.to(device=DEVICE, dtype=torch.long)\n            logit_mask = model(X)\n            loss = loss_fn(logit_mask, y.squeeze())\n            val_loss += loss.item()\n            \n            prob_mask = logit_mask.softmax(dim=1)\n            pred_mask = prob_mask.argmax(dim=1)\n            \n            tp, fp, fn, tn = smp.metrics.get_stats(\n                output=pred_mask.detach().cpu().long(),\n                target=y.squeeze().cpu().long(),\n                mode=\"multiclass\",\n                num_classes=22\n            )\n            \n            val_accuracy += smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").numpy()\n    \n    val_loss /= len(dataloader)\n    val_accuracy /= len(dataloader)\n    return val_loss, val_accuracy\n\ndef train_model(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, \n               val_dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer, early_stopping, epochs: int = 10):\n    results = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_accuracy = train_step(model, train_dataloader, loss_fn, optimizer)\n        val_loss, val_accuracy = val_step(model, val_dataloader, loss_fn)\n        \n        print(f'Epoch: {epoch + 1} | '\n              f'Train Loss: {train_loss:.4f} | '\n              f'Train Accuracy: {train_accuracy:.4f} | '\n              f'Val Loss: {val_loss:.4f} | '\n              f'Val Accuracy: {val_accuracy:.4f}')\n        \n        early_stopping(val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early Stopping triggered!\")\n            break\n            \n        results['train_loss'].append(train_loss)\n        results['train_accuracy'].append(train_accuracy)\n        results['val_loss'].append(val_loss)\n        results['val_accuracy'].append(val_accuracy)\n        \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:48.008682Z","iopub.execute_input":"2025-01-06T18:54:48.008933Z","iopub.status.idle":"2025-01-06T18:54:48.021924Z","shell.execute_reply.started":"2025-01-06T18:54:48.008911Z","shell.execute_reply":"2025-01-06T18:54:48.021112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CUDA\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:48.022742Z","iopub.execute_input":"2025-01-06T18:54:48.022958Z","iopub.status.idle":"2025-01-06T18:54:48.033994Z","shell.execute_reply.started":"2025-01-06T18:54:48.022940Z","shell.execute_reply":"2025-01-06T18:54:48.033188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Set Seeds for Reproducibility\nSEED = 42\nEPOCHS = 50  # Adjust as needed\ntorch.cuda.manual_seed(SEED)\ntorch.manual_seed(SEED)\n\n# Define Loss Function and Optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n\n# Train the Model\nRESULTS = train_model(\n    model=model.to(DEVICE),\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    loss_fn=loss_fn,\n    optimizer=optimizer,\n    early_stopping=early_stopping,\n    epochs=EPOCHS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:54:48.035144Z","iopub.execute_input":"2025-01-06T18:54:48.035745Z","iopub.status.idle":"2025-01-06T20:00:43.554535Z","shell.execute_reply.started":"2025-01-06T18:54:48.035713Z","shell.execute_reply":"2025-01-06T20:00:43.553545Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Evaluation","metadata":{}},{"cell_type":"code","source":"# Plot Loss and Accuracy\ndef loss_and_metric_plot(results: dict):\n    training_loss = results['train_loss']\n    training_metric = results['train_accuracy']\n    validation_loss = results['val_loss']\n    validation_metric = results['val_accuracy']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n    \n    # Plot Loss\n    ax[0].plot(training_loss, label=\"Train Loss\")\n    ax[0].plot(validation_loss, label=\"Val Loss\")\n    ax[0].set_title(\"CrossEntropyLoss\", fontsize=12, fontweight=\"bold\")\n    ax[0].set_xlabel(\"Epoch\", fontsize=10, fontweight=\"bold\")\n    ax[0].set_ylabel(\"Loss\", fontsize=10, fontweight=\"bold\")\n    ax[0].legend()\n    \n    # Plot Accuracy\n    ax[1].plot(training_metric, label=\"Train Accuracy\")\n    ax[1].plot(validation_metric, label=\"Val Accuracy\")\n    ax[1].set_title(\"Accuracy\", fontsize=12, fontweight=\"bold\")\n    ax[1].set_xlabel(\"Epoch\", fontsize=10, fontweight=\"bold\")\n    ax[1].set_ylabel(\"Accuracy\", fontsize=10, fontweight=\"bold\")\n    ax[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nloss_and_metric_plot(RESULTS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:00:43.555860Z","iopub.execute_input":"2025-01-06T20:00:43.556120Z","iopub.status.idle":"2025-01-06T20:00:44.084881Z","shell.execute_reply.started":"2025-01-06T20:00:43.556095Z","shell.execute_reply":"2025-01-06T20:00:44.083896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path_test = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS/test\"\n\nIMAGE_PATH_LIST_TEST = list(Path(image_path_test).glob(\"*.png\"))\n\nprint(f'Total Images Train: {len(IMAGE_PATH_LIST_TEST)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:00:44.086157Z","iopub.execute_input":"2025-01-06T20:00:44.086426Z","iopub.status.idle":"2025-01-06T20:00:44.261294Z","shell.execute_reply.started":"2025-01-06T20:00:44.086403Z","shell.execute_reply":"2025-01-06T20:00:44.260497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTestDataset(Dataset):\n    def __init__(self, data:pd.DataFrame, image_transforms):\n        self.data = data\n        self.image_transforms = image_transforms\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        image_path = self.data.iloc[idx, 0]\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.image_transforms(image)\n        \n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:19:21.535548Z","iopub.execute_input":"2025-01-06T20:19:21.536404Z","iopub.status.idle":"2025-01-06T20:19:21.544561Z","shell.execute_reply.started":"2025-01-06T20:19:21.536351Z","shell.execute_reply":"2025-01-06T20:19:21.543335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictions_mask(test_dataloader: torch.utils.data.DataLoader):\n    # Load the best model\n    checkpoint = torch.load(\"/kaggle/working/best_model.pth\")\n    \n    # Initialize UNet++ model with the correct number of classes\n    loaded_model = smp.UnetPlusPlus(\n        encoder_name=\"resnet34\",\n        encoder_weights=None,  # weights are loaded from checkpoint\n        in_channels=3,\n        classes=22,  # Updated to match checkpoint\n        activation=None\n    )\n    \n    # Load state dict\n    loaded_model.load_state_dict(checkpoint)\n    \n    # Move to device\n    loaded_model.to(device=DEVICE)\n    \n    # Set to evaluation mode\n    loaded_model.eval()\n    \n    y_pred_mask = []\n    \n    with torch.inference_mode():\n        for batch, X in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n            X = X.to(device=DEVICE, dtype=torch.float32)\n            mask_logit = loaded_model(X)\n            mask_prob = mask_logit.softmax(dim=1)\n            mask_pred = mask_prob.argmax(dim=1)\n            y_pred_mask.append(mask_pred.detach().cpu())\n    \n    y_pred_mask = torch.cat(y_pred_mask)\n    image_path_test = \"/kaggle/input/breast-cancer-semantic-segmentation-bcss/BCSS/test\"\n    IMAGE_PATH_LIST_TEST = list(Path(image_path_test).glob(\"*.png\"))\n    print(f'Total Images Test: {len(IMAGE_PATH_LIST_TEST)}')\n    return y_pred_mask\n\n# Prepare Test Data\ndata_test = pd.DataFrame({'Image': IMAGE_PATH_LIST_TEST})\ndata_test.head()\n\n# Create Test Dataset and DataLoader\ntest_dataset = CustomTestDataset(data_test, image_transforms)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n# Execute Predictions\ny_pred_mask = predictions_mask(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:26:48.931449Z","iopub.execute_input":"2025-01-06T20:26:48.932193Z","iopub.status.idle":"2025-01-06T20:27:22.905758Z","shell.execute_reply.started":"2025-01-06T20:26:48.932164Z","shell.execute_reply":"2025-01-06T20:27:22.904745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Original Image, Ground Truth Mask (Unprocessed), and Predicted Mask\nfig, ax = plt.subplots(nrows=10, ncols=3, figsize=(18, 35))\n\nfor index, row in data_test.iterrows():\n    if index > 9:  # Limit to 10 samples\n        break\n    \n    # Original Image\n    img_bgr = cv2.imread(str(row[0]))\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    ax[index, 0].imshow(img_rgb)\n    ax[index, 0].axis('off')\n    ax[index, 0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\", color=\"black\")\n    \n    # Ground Truth Mask (Unprocessed)\n    if index < len(MASK_PATH_LIST_TRAIN):  # Ensure we are accessing the correct index\n        original_mask_path = MASK_PATH_LIST_TRAIN[index]\n        ground_truth_mask = cv2.imread(str(original_mask_path), cv2.IMREAD_GRAYSCALE)  # Load original mask\n        ax[index, 1].imshow(ground_truth_mask)\n        ax[index, 1].axis('off')\n        ax[index, 1].set_title(\"Ground Truth Mask\", fontsize=12, fontweight=\"bold\", color=\"black\")\n    else:\n        ax[index, 1].axis('off')  # Leave blank if ground truth is not available\n\n    # Predicted Mask\n    ax[index, 2].imshow(y_pred_mask[index].squeeze().numpy(), cmap='jet')\n    ax[index, 2].axis('off')\n    ax[index, 2].set_title(\"Predicted Mask\", fontsize=12, fontweight=\"bold\", color=\"black\")\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:29:02.724227Z","iopub.execute_input":"2025-01-06T20:29:02.724608Z","iopub.status.idle":"2025-01-06T20:29:06.330854Z","shell.execute_reply.started":"2025-01-06T20:29:02.724575Z","shell.execute_reply":"2025-01-06T20:29:06.329786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize 5 Original Images, Ground Truth Masks, and Predicted Masks\nfig, ax = plt.subplots(nrows=4, ncols=3, figsize=(18, 25))\n\nfor index, row in data_test.iterrows():\n    if index > 3:  # Limit to 5 samples\n        break\n    \n    # Original Image\n    img_bgr = cv2.imread(str(row[0]))\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    ax[index, 0].imshow(img_rgb)\n    ax[index, 0].axis('off')\n    ax[index, 0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\", color=\"black\")\n    \n    # Ground Truth Mask (Unprocessed)\n    if index < len(MASK_PATH_LIST_TRAIN):  # Ensure we are accessing the correct index\n        original_mask_path = MASK_PATH_LIST_TRAIN[index]\n        ground_truth_mask = cv2.imread(str(original_mask_path), cv2.IMREAD_GRAYSCALE)  # Load original mask\n        ax[index, 1].imshow(ground_truth_mask)\n        ax[index, 1].axis('off')\n        ax[index, 1].set_title(\"Ground Truth Mask\", fontsize=12, fontweight=\"bold\", color=\"black\")\n    else:\n        ax[index, 1].axis('off')  # Leave blank if ground truth is not available\n\n    # Predicted Mask\n    ax[index, 2].imshow(y_pred_mask[index].squeeze().numpy(), cmap='jet')\n    ax[index, 2].axis('off')\n    ax[index, 2].set_title(\"Predicted Mask\", fontsize=12, fontweight=\"bold\", color=\"black\")\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:29:38.074741Z","iopub.execute_input":"2025-01-06T20:29:38.075322Z","iopub.status.idle":"2025-01-06T20:29:40.578943Z","shell.execute_reply.started":"2025-01-06T20:29:38.075293Z","shell.execute_reply":"2025-01-06T20:29:40.577705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\nimport torch.nn as nn\n\n# Assuming the following variables are already defined:\n# DEVICE, CustomTestDataset, image_transforms, BATCH_SIZE, NUM_WORKERS,\n# IMAGE_PATH_LIST_TEST, MASK_PATH_LIST_TRAIN, y_pred_mask\n\ndef visualize_predictions(data_test, y_pred_mask, mask_path_list_train, num_samples=5):\n    \"\"\"\n    Visualize Original Images, Ground Truth Masks, and Predicted Masks in rows.\n\n    Parameters:\n    - data_test (pd.DataFrame): DataFrame containing test image paths.\n    - y_pred_mask (torch.Tensor): Tensor containing predicted masks.\n    - mask_path_list_train (list): List of ground truth mask paths.\n    - num_samples (int): Number of samples to visualize.\n    \"\"\"\n    # Ensure num_samples does not exceed available samples\n    num_samples = min(num_samples, len(data_test), len(y_pred_mask), len(mask_path_list_train))\n    \n    # Select the first 'num_samples' indices\n    selected_indices = list(range(num_samples))\n    \n    # Create a subplot grid with 3 rows and 'num_samples' columns\n    fig, axes = plt.subplots(nrows=3, ncols=num_samples, figsize=(5 * num_samples, 15))\n    \n    # If there's only one sample, axes might not be a 2D array\n    if num_samples == 1:\n        axes = axes.reshape(3, 1)\n    \n    for col, idx in enumerate(selected_indices):\n        # --- Row 1: Original Image ---\n        img_path = data_test.iloc[idx]['Image']  # Ensure 'Image' column exists\n        img_bgr = cv2.imread(str(img_path))\n        if img_bgr is None:\n            print(f\"Warning: Image at {img_path} could not be loaded.\")\n            axes[0, col].axis('off')\n            axes[0, col].set_title(f\"Sample {idx+1}\\nOriginal Image\\n(Not Available)\", fontsize=12, fontweight=\"bold\")\n        else:\n            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n            axes[0, col].imshow(img_rgb)\n            axes[0, col].axis('off')\n            axes[0, col].set_title(f\"Sample {idx+1}\\nOriginal Image\", fontsize=12, fontweight=\"bold\")\n        \n        # --- Row 2: Ground Truth Mask ---\n        if idx < len(mask_path_list_train):\n            mask_path = mask_path_list_train[idx]\n            ground_truth_mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n            if ground_truth_mask is not None:\n                axes[1, col].imshow(ground_truth_mask)\n                axes[1, col].axis('off')\n                axes[1, col].set_title(f\"Sample {idx+1}\\nGround Truth Mask\", fontsize=12, fontweight=\"bold\")\n            else:\n                axes[1, col].axis('off')\n                axes[1, col].set_title(f\"Sample {idx+1}\\nGround Truth Mask\\n(Not Available)\", fontsize=12, fontweight=\"bold\")\n        else:\n            axes[1, col].axis('off')\n            axes[1, col].set_title(f\"Sample {idx+1}\\nGround Truth Mask\\n(Not Available)\", fontsize=12, fontweight=\"bold\")\n        \n        # --- Row 3: Predicted Mask ---\n        if idx < len(y_pred_mask):\n            predicted_mask = y_pred_mask[idx].squeeze().numpy()\n            # If the predicted mask has been processed differently, adjust accordingly\n            axes[2, col].imshow(predicted_mask, cmap='jet')\n            axes[2, col].axis('off')\n            axes[2, col].set_title(f\"Sample {idx+1}\\nPredicted Mask\", fontsize=12, fontweight=\"bold\")\n        else:\n            axes[2, col].axis('off')\n            axes[2, col].set_title(f\"Sample {idx+1}\\nPredicted Mask\\n(Not Available)\", fontsize=12, fontweight=\"bold\")\n    \n    # Adjust layout for better spacing\n    plt.tight_layout()\n    plt.show()\n\n# Example Usage:\n\n# Prepare Test Data\ndata_test = pd.DataFrame({'Image': IMAGE_PATH_LIST_TEST})\ndata_test.head()\n\n# Create Test Dataset and DataLoader\ntest_dataset = CustomTestDataset(data_test, image_transforms)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\n# Execute Predictions\ny_pred_mask = predictions_mask(test_dataloader)\n\n# Visualize Predictions\nvisualize_predictions(\n    data_test=data_test,\n    y_pred_mask=y_pred_mask,\n    mask_path_list_train=MASK_PATH_LIST_TRAIN,\n    num_samples=5  # Adjust the number of samples as needed\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T20:36:58.893642Z","iopub.execute_input":"2025-01-06T20:36:58.894279Z","iopub.status.idle":"2025-01-06T20:37:38.170405Z","shell.execute_reply.started":"2025-01-06T20:36:58.894239Z","shell.execute_reply":"2025-01-06T20:37:38.169180Z"}},"outputs":[],"execution_count":null}]}